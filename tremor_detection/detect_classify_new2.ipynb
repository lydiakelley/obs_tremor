{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a888b941-efed-43b6-8790-edcbfec55efc",
   "metadata": {},
   "source": [
    "# **Detect and Classify Tremor from the IRIS Database**\n",
    "---\n",
    "Workflow:\n",
    "- Imports\n",
    "- Inputs\n",
    "- Define Functions\n",
    "- Define Detect and Classify Function\n",
    "- Bin by Days\n",
    "- Apply Function & Process in Parallel \n",
    "- Concat Results & Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7266f-5c80-435b-a0a7-edf1ea6f1266",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51f65d0-53ac-475c-adad-55711c39e791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from obspy.clients.fdsn.client import Client\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "#client = Client('IRIS')\n",
    "import pickle\n",
    "import os\n",
    "import scipy.signal\n",
    "import scipy.ndimage\n",
    "\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from obspy.signal.trigger import trigger_onset\n",
    "from obspy import UTCDateTime\n",
    "from obspy import Stream\n",
    "\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from obspy import Stream\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "from obspy.clients.fdsn import Client\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92181647-f454-4b4c-a1e3-b46a4c08df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets data locally rather than online\n",
    "client = WaveformClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57da48a-b42d-42cf-beea-307481de34a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inputs\n",
    "Short Term Average & Long Term Average (STA/LTA) parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44149b4f-61fb-45e3-9afe-07051adf8b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define step we want to chunk\n",
    "step = 10 # seconds\n",
    "\n",
    "# Define sta and lta window lengths\n",
    "# Below chosen to optimize emergent signal detection!\n",
    "\n",
    "sta_win = 10 # seconds, short term window\n",
    "lta_win = 1000 # seconds, long term window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725151f-fbad-44b3-9b5e-c026be81024d",
   "metadata": {},
   "source": [
    "Time Endpoints and Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45760faa-0bff-4716-85b5-b259dd4b3f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = obspy.UTCDateTime(\"2013-09-06T00:00:00.000\")\n",
    "t2 = obspy.UTCDateTime(\"2014-06-24T00:00:00.000\")\n",
    "\n",
    "network = '7D'\n",
    "station = 'J34C' \n",
    "channel = 'HH2'\n",
    "location = '*'\n",
    "\n",
    "# Get response info for this station from IRIS\n",
    "iris_client = Client('IRIS')\n",
    "sta_response = iris_client.get_stations(network=network, station=station,starttime=t1,endtime=t2,level='response')\n",
    "save_dir = 'classifications/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d083c9a-c008-4d83-a320-4d1489092897",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "- Detect/Triggering: Check Data, Calculate STL/STA\n",
    "- Classify: Pick Peaks, Apply Gaussian, Ship Noise\n",
    "- Attach Response Cascadia\n",
    "- Waveform across Midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d868135-d70c-4f67-8893-4c34227840d8",
   "metadata": {},
   "source": [
    "Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ae8174-adeb-4b7d-9d66-9345603990d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(stream,t1,sr,step,data_time_length):\n",
    "    \n",
    "    data = stream[0].data\n",
    "    data_sample_length = sr * data_time_length\n",
    "\n",
    "    \n",
    "    # Catch for if data stream is less than specified\n",
    "    if len(data) < data_sample_length:\n",
    "        data_sample_length = int(len(data) - (len(data)%(sr*step)))\n",
    "        data_time_length = int(data_sample_length / sr)\n",
    "        \n",
    "    # Catch for if start time is not as specified\n",
    "    if stream[0].stats.starttime != t1:\n",
    "        t1 = stream[0].stats.starttime\n",
    "    \n",
    "    return(t1,data_time_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d6566-beda-4a4b-aa84-d5ba662593c4",
   "metadata": {},
   "source": [
    "Calculate Short Term Average & Long Term Average (STL/STA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40deea41-47be-44b8-96a1-6995a5d739b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stalta(data,sr,step,data_time_length,sta_win,lta_win):\n",
    "    \n",
    "    # Matricize the data as step s chunks\n",
    "    data_sample_length = sr * data_time_length\n",
    "    \n",
    "    \n",
    "    chunked = np.reshape(data[0:data_sample_length],[int(data_sample_length/(sr*step)),int(sr*step)])\n",
    "    chunked_medians = [np.median(chunked[i,:]) for i in range(np.shape(chunked)[0])]\n",
    "    chunked_times = np.linspace(0,data_time_length,len(chunked_medians))\n",
    "    \n",
    "    # Step through and calculate sta & lta every step s\n",
    "    sta = []\n",
    "    lta = []\n",
    "    for i,vec in enumerate(chunked_medians):\n",
    "\n",
    "        # STA is median of the next window\n",
    "        nwin = int(sta_win / step)\n",
    "        sta.append(np.median(chunked_medians[i:i+nwin]))\n",
    "\n",
    "\n",
    "        # LTA is median of the past window\n",
    "        nwin = int(lta_win / step)\n",
    "        lta.append(np.median(chunked_medians[i-nwin:i]))\n",
    "    \n",
    "    stalta = np.array(sta)/np.array(lta)\n",
    "    \n",
    "    return(stalta,chunked_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91529f2-fbbc-4f94-b278-be18da65d277",
   "metadata": {},
   "source": [
    "Pick Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b84d625-3154-4f25-9fe3-395720c74e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_peaks_welch(trace,sampling_rate,nperseg_multiple,microseism_cutoff=True):\n",
    "    \"\"\"\n",
    "    Estimate power spectra of a trace using Welch's method\n",
    "    Pick peaks within the spectra!\n",
    "    \n",
    "    INPUTS:\n",
    "    trace = obspy object, waveform\n",
    "    sampling_rate = sampling rate of trace\n",
    "    nperseg_multiple = length of each segment used to construct the Welch spectrum\n",
    "    microseism_cutoff = Bool, whether or not to cut off the lower end of the spectrum to avoid the microseism\n",
    "    \n",
    "    OUTPUTS:\n",
    "    f = frequencies of the spectra\n",
    "    Pxx_den = associated power at each frequency, in decibels\n",
    "    peak_ind = index of peaks within the spectra (f and Pxx_den), if found\n",
    "    peaks = picked peak object from scipy\n",
    "    median_power = median power of spectra from 20-80 Hz in decibels\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    fs = sampling_rate\n",
    "    x = trace.data\n",
    "    nperseg = fs * nperseg_multiple\n",
    "    \n",
    "    f,Pxx_den = scipy.signal.welch(x,fs,nperseg=nperseg)\n",
    "    if microseism_cutoff is True:\n",
    "        f = f[4:]\n",
    "        Pxx_den = Pxx_den[4:]\n",
    "        \n",
    "    Pxx_den = [10*np.log10(d) for d in Pxx_den]\n",
    "    median_power = np.median(Pxx_den[20:80])\n",
    "    \n",
    "    peaks = scipy.signal.find_peaks(Pxx_den,threshold =median_power*5,prominence=10) \n",
    "    peak_ind = peaks[0]\n",
    "    \n",
    "    return(f,Pxx_den,peak_ind,peaks,median_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef26dd7-fed4-4f18-90e9-2abfac875c71",
   "metadata": {},
   "source": [
    "Apply Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b20a01-f28b-45ce-b4ed-b65138fa537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian(filtered_data,samp_rate,gaussian_width=5):\n",
    "    \"\"\"\n",
    "    Smooth waveform using a gaussian window\n",
    "    \n",
    "    INPUTS\n",
    "    filtered_data = filtered numpy array of seismic data (from an obspy trace)\n",
    "    samp_rate = sampling rate of data\n",
    "    gaussian width = width of Gaussian window in seconds\n",
    "    \n",
    "    OUTPUTS\n",
    "    smoothed_window = smoothed numpy array of seismic data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Square data\n",
    "    data = filtered_data**2\n",
    "    \n",
    "    gaussian_radius = int((gaussian_width * samp_rate)/2)\n",
    "    smoothed_window=scipy.ndimage.gaussian_filter1d(data,sigma=gaussian_radius/4,radius=gaussian_radius)\n",
    "    \n",
    "    return smoothed_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfade60-bf46-4137-99bd-505382edc98c",
   "metadata": {},
   "source": [
    "Ship Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb945d3-8411-4be0-b548-9a5478e16304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ship_noise_classifier(trace,sampling_rate):\n",
    "    \"\"\"\n",
    "    Check whether detection likely includes ship noise in the form of a spectral peak\n",
    "    \n",
    "    INPUTS\n",
    "    trace = obspy trace object\n",
    "    sampling_rate = sample rate of trace\n",
    "    \n",
    "    OUTPUTS\n",
    "    ship_classifier = number of peaks in the spectra. If any exist, ship noise is likely!\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Pick peaks on the smoothed spectrum of the trace (nperseg multiple = 1)\n",
    "    f,Pxx_den,peak_ind,peak_details,median_power = pick_peaks_welch(trace,sampling_rate,1,microseism_cutoff=True)\n",
    "    \n",
    "    if len(peak_ind)==0:\n",
    "        ship_classifier = 0\n",
    "    else:\n",
    "        ship_classifier = len(peak_ind)\n",
    "\n",
    "    \n",
    "    return ship_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3820442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: SHOULDN'T NEED THIS ANYMORE\n",
    "# PULL UP-TO-DATE STATION INFO FROM IRIS AT TOP OF NOTEBOOK\n",
    "\n",
    "# def attach_response_cascadia(stream):\n",
    "#     \"\"\"\n",
    "#     Reads in an obspy stream\n",
    "#     Pulls down station response information for the corresponding station from where it is stored in Cascadia\n",
    "#     Attaches this station response information to the stream, and returns the stream\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # This is where station response information is stored on Cascadia, in xml format:\n",
    "#     dir_base = '/1-fnp/pnwstore1/p-wd11/PNWStationXML/'\n",
    "    \n",
    "#     network = stream[0].stats.network\n",
    "#     station = stream[0].stats.station\n",
    "    \n",
    "#     # Response file paths are in the format /1-fnp/pnwstore1/p-wd11/PNWStationXML/OO/OO.HYSB1.xml\n",
    "#     response_file = dir_base+network+'/'+network+'.'+station+'.xml'\n",
    "\n",
    "#     # Read response info using obspy\n",
    "#     inventory = obspy.read_inventory(response_file)\n",
    "\n",
    "#     # Attach response information to stream\n",
    "#     stream.attach_response(inventory)\n",
    "    \n",
    "#     return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a16c746-cb39-4726-b4bf-e8b736384ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_waveform_across_midnight(client, network, station, location, channel, starttime, endtime):\n",
    "    stream = Stream()\n",
    "    current = starttime\n",
    "    while current < endtime:\n",
    "        end_of_day = obspy.UTCDateTime(current.date) + 86399  # 23:59:59\n",
    "        chunk_end = min(end_of_day, endtime)\n",
    "        #print(current, chunk_end)\n",
    "        #try:\n",
    "        st_chunk = client.get_waveforms(\n",
    "            network=network, station=station,\n",
    "            location=location, channel=channel,\n",
    "            starttime=current, endtime=chunk_end,\n",
    "        )\n",
    "        sr = round(st_chunk[0].stats.sampling_rate)\n",
    "        st_chunk.resample(sr).merge(fill_value=\"interpolate\")\n",
    "        stream += st_chunk\n",
    "        #except Exception as e:\n",
    "        #    print(f\"Failed to get data from {current} to {chunk_end}: {e}\")\n",
    "        current = obspy.UTCDateTime(current.date + timedelta(days=1))\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d695d-fb65-4b98-bdbb-71818a132467",
   "metadata": {},
   "source": [
    "## Define Detect and Classify Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff6e6f9b-cea7-49c5-b7d3-8e35dd3d009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_and_classify(t1, data_time_length, network, station, channel, sta_response, step, sta_win, lta_win, filepath):\n",
    "    \"\"\"\n",
    "    Detects and classifies seismic events using STA/LTA and signal features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Adjust times with padding\n",
    "    t1 = obspy.UTCDateTime(t1)\n",
    "    t2 = t1 + data_time_length + 1000\n",
    "    t1 = t1 - 1000\n",
    "\n",
    "    try:\n",
    "        client = WaveformClient() \n",
    "\n",
    "        # === 1. Detection ===\n",
    "        st1 = get_waveform_across_midnight(client, network, station, \"*\", channel, t1, t2)\n",
    "        st1.merge(fill_value= \"interpolate\")\n",
    "        #print(f\"Stream length: {len(st1)}\")\n",
    "        if len(st1) == 0:\n",
    "            print(f\"[{t1}] No traces found in stream. Skipping.\")\n",
    "            return [], []\n",
    "\n",
    "        sr = round(st1[0].stats.sampling_rate)\n",
    "        if st1[0].stats.sampling_rate != sr:\n",
    "            st1.resample(sr).merge(fill_value=\"interpolate\")\n",
    "        st1.taper(0.05, max_length=5)\n",
    "        st1.filter(\"bandpass\", freqmin=3, freqmax=10)\n",
    "        st1.trim(starttime=t1, endtime=t2)\n",
    "\n",
    "        t1, data_time_length = check_data(st1, t1, sr, step, data_time_length)\n",
    "        data = np.abs(st1[0].data)\n",
    "        #print(f\"Analyzing {st1[0].stats.station} {st1[0].stats.channel}\")\n",
    "        stalta, times = calc_stalta(data, sr, step, data_time_length, sta_win, lta_win)\n",
    "        triggers = trigger_onset(stalta, 2, 1)\n",
    "        ontimes = [t1 + times[tr[0]] for tr in triggers]\n",
    "        offtimes = [t1 + times[tr[1]] for tr in triggers] \n",
    "        # Get rid of any detections less than 30 s\n",
    "        keep_ind = []\n",
    "        for j in range(len(ontimes)):\n",
    "            if offtimes[j]-ontimes[j] > 30:\n",
    "                keep_ind.append(j)\n",
    "        ontimes = [ontimes[j] for j in keep_ind]\n",
    "        offtimes = [offtimes[j] for j in keep_ind]\n",
    "        # print(ontimes,offtimes)\n",
    "\n",
    "        # === 2. Classification ===\n",
    "        all_results = []\n",
    "\n",
    "        for on, off in zip(ontimes, offtimes):\n",
    "            #try:\n",
    "            t1c, t2c = on, off\n",
    "\n",
    "            # Ship noise classifier\n",
    "            st_ship = get_waveform_across_midnight(client, network, station, \"*\", channel, t1c-5, t2c+5)\n",
    "            st_ship.resample(sr).merge(fill_value=\"interpolate\")\n",
    "            # st_ship = attach_response_cascadia(st_ship)\n",
    "            st_ship.attach_response(sta_response)\n",
    "            st_ship[0].data = st_ship[0].data / st_ship[0].stats.response.instrument_sensitivity.value\n",
    "            st_ship.trim(starttime=t1c, endtime=t2c)\n",
    "            ship_classifier = ship_noise_classifier(st_ship[0], sr)\n",
    "\n",
    "            # Waveform peak classifier\n",
    "            st_wave = get_waveform_across_midnight(client, network, station, \"*\", channel, t1c-5, t2c+5)\n",
    "            st_wave.resample(sr).merge(fill_value=\"interpolate\")\n",
    "            # st_wave = attach_response_cascadia(st_wave)\n",
    "            st_wave.attach_response(sta_response)\n",
    "            st_wave.filter(\"bandpass\", freqmin=3, freqmax=10)\n",
    "            st_wave.remove_response()\n",
    "            st_wave.trim(starttime=t1c, endtime=t2c)\n",
    "            max_amplitude = np.max(np.abs(st_wave[0].data))\n",
    "            smoothed = apply_gaussian(st_wave[0].data, sr, gaussian_width=15)\n",
    "            smoothed = smoothed / np.max(smoothed)\n",
    "            peaks = scipy.signal.find_peaks(smoothed, prominence=0.1)\n",
    "            num_waveform_peaks = len(peaks[0])\n",
    "\n",
    "            # Welch frequency ratio\n",
    "            t1w = t1c - 30\n",
    "            t2w = t2c + 30\n",
    "            st_welch = get_waveform_across_midnight(client, network, station, \"*\", channel, t1w-5, t2w+5)\n",
    "            st_welch.resample(sr).merge(fill_value=\"interpolate\")\n",
    "            # st_welch = attach_response_cascadia(st_welch)\n",
    "            st_welch.attach_response(sta_response)\n",
    "            st_welch[0].data = st_welch[0].data / st_welch[0].stats.response.instrument_sensitivity.value\n",
    "            st_welch.trim(starttime=t1w, endtime=t2w)\n",
    "            f, Pxx_den, *_ = pick_peaks_welch(st_welch[0], sr, 5, microseism_cutoff=False)\n",
    "            freq_ratio_welch = 10 ** (np.median(Pxx_den[25:50]) / 10) / 10 ** (np.median(Pxx_den[50:75]) / 10)\n",
    "\n",
    "            # Save result\n",
    "            all_results.append([\n",
    "                (t1c, t2c), station, num_waveform_peaks,\n",
    "                ship_classifier, freq_ratio_welch, max_amplitude\n",
    "            ])\n",
    "\n",
    "            #except Exception as e:\n",
    "                #print(f\"Classification failed for detection starting at {str(on)}: {e}\")\n",
    "                #continue\n",
    "\n",
    "        # === Save all results ===\n",
    "        # combined_file = filepath + station + \"_all_detections.pickle\"\n",
    "        # with open(combined_file, \"wb\") as handle:\n",
    "        #     pickle.dump(all_results, handle)\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{str(t1)}] No data or fatal error during detection/classification: {e}\")\n",
    "        return [], []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1966b1-e41c-4053-ae2b-a5ed274abade",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bin data by days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eea298b8-eba1-42fa-8950-07a5d69cb45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_bins = pd.date_range(start=t1.datetime, end=t2.datetime, freq='D')\n",
    "data_time_length = 24 * 60 * 60\n",
    "pickle_path = 'classifications/J34C_raw.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9578a37-2218-475c-971e-fee0fa7c2c46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 5.04 s ms[2014-05-21T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[                                        ] | 0% Completed | 6.16 s[2014-06-04T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[                                        ] | 0% Completed | 16.57 s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter_share/miniconda3/envs/seismo2/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/jupyter_share/miniconda3/envs/seismo2/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 2% Completed | 54.77 s[2014-04-09T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[##                                      ] | 6% Completed | 143.77 s[2014-06-07T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[###                                     ] | 7% Completed | 146.02 s[2013-10-04T23:43:19.999800Z] No data or fatal error during detection/classification: list index out of range\n",
      "[########                                ] | 20% Completed | 204.71 s[2013-10-28T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[########                                ] | 22% Completed | 224.73 s[2013-10-27T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[#########                               ] | 23% Completed | 242.62 s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " WARNING (norm_resp): computed and reported sensitivities differ by more than 5 percent. \n",
      "\t Execution continuing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##########                              ] | 25% Completed | 259.03 s[2013-10-30T23:43:20.003700Z] No data or fatal error during detection/classification: list index out of range\n",
      "[###########                             ] | 29% Completed | 286.32 s[2014-04-10T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[#############                           ] | 32% Completed | 303.29 s[2014-05-20T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[#############                           ] | 34% Completed | 313.02 s[2013-09-05T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[##############                          ] | 36% Completed | 340.54 s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " WARNING (norm_resp): computed and reported sensitivities differ by more than 5 percent. \n",
      "\t Execution continuing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###############                         ] | 39% Completed | 361.36 s[2013-11-08T23:43:19.999200Z] No data or fatal error during detection/classification: list index out of range\n",
      "[###################                     ] | 47% Completed | 422.78 s[2013-11-05T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[###################                     ] | 49% Completed | 431.97 s[2013-11-06T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[####################                    ] | 50% Completed | 443.61 s[2014-06-08T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[#####################                   ] | 54% Completed | 462.78 s[2014-06-01T23:43:20.002300Z] No data or fatal error during detection/classification: list index out of range\n",
      "[##########################              ] | 65% Completed | 10m 3s s[2013-10-20T23:43:20.001600Z] No data or fatal error during detection/classification: list index out of range\n",
      "[##########################              ] | 66% Completed | 10m 4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " WARNING (norm_resp): computed and reported sensitivities differ by more than 5 percent. \n",
      "\t Execution continuing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##########################              ] | 66% Completed | 10m 21s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###########################             ] | 69% Completed | 11m 31s[2014-06-19T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[#############################           ] | 72% Completed | 12m 33s[2014-06-03T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[##############################          ] | 75% Completed | 12m 57s[2014-06-23T23:43:20.000000Z] No data or fatal error during detection/classification: list index out of range\n",
      "[##################################      ] | 86% Completed | 14m 56s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " WARNING (norm_resp): computed and reported sensitivities differ by more than 5 percent. \n",
      "\t Execution continuing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################################    ] | 92% Completed | 15m 28s[2014-06-20T23:54:10.135900Z] No data or fatal error during detection/classification: list index out of range\n",
      "[#####################################   ] | 94% Completed | 15m 32s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " WARNING (norm_resp): computed and reported sensitivities differ by more than 5 percent. \n",
      "\t Execution continuing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################################### ] | 98% Completed | 15m 39s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " WARNING (norm_resp): computed and reported sensitivities differ by more than 5 percent. \n",
      "\t Execution continuing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 15m 40s\n"
     ]
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def loop_days(t, data_time_length, network, station, channel, sta_reseponse, step, sta_win, lta_win, filepath):\n",
    "    t_utc = obspy.UTCDateTime(t)\n",
    "    return trigger_and_classify(t_utc, data_time_length, network, station, channel, sta_response, step, sta_win, lta_win, filepath)\n",
    "\n",
    "# Build lazy results list\n",
    "lazy_results = [\n",
    "    loop_days(t, data_time_length, network, station, channel, sta_response, step, sta_win, lta_win, save_dir)\n",
    "    for t in time_bins\n",
    "]\n",
    "\n",
    "# Run Dask\n",
    "with ProgressBar():\n",
    "    results = dask.compute(*lazy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92851b38-2b9e-4a37-babd-c6cd35ffce6b",
   "metadata": {},
   "source": [
    "## Concat Results & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9fd8d9-a8b2-439c-b9ec-82cfd51945ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/lydiagk/obs_tremor/tremor_detection\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/lydiagk/obs_tremor/tremor_detection')\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02cb33aa-5bfb-4ac6-9585-4de066d61ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined detection results to classifications/J34C_raw.pickle\n",
      "Total detections: 292\n"
     ]
    }
   ],
   "source": [
    "# removing empty brackets\n",
    "cleaned_results = [[entry] for entry in results if entry]\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(cleaned_results, f)\n",
    "\n",
    "print(f\"Saved combined detection results to {pickle_path}\")\n",
    "print(f\"Total detections: {len(cleaned_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3cc8733-8099-4ef5-acc9-1da3510faf76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(pickle_path, 'rb') as f:\n",
    "    raw_triggers = pickle.load(f)\n",
    "\n",
    "flattened = [\n",
    "    detection\n",
    "    for day in raw_triggers\n",
    "    if isinstance(day, list)\n",
    "    for detection in day\n",
    "    if detection and isinstance(detection, (list, tuple)) and len(detection) == 6]\n",
    "\n",
    "with open('classifications/J34C.pickle', 'wb') as f:\n",
    "    pickle.dump(flattened, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4768e5e-f521-45e8-aa7b-365b32df1fe1",
   "metadata": {},
   "source": [
    "## Checking my work against validated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be4c260b-5fbc-4d38-a71e-fd128c17c332",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'flat_triggers.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m UTCDateTime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2017-09-01T00:00:00.000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m end_time \u001b[38;5;241m=\u001b[39m UTCDateTime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2017-09-08T00:00:00.000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflat_triggers.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     my_triggers \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m my_triggers \u001b[38;5;28;01mif\u001b[39;00m start_time \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_time)\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'flat_triggers.pickle'"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/lydiagk/obs_tremor/tremor_detection\")\n",
    "\n",
    "\n",
    "start_time = UTCDateTime(\"2017-09-01T00:00:00.000\")\n",
    "end_time = UTCDateTime(\"2017-09-08T00:00:00.000\")\n",
    "\n",
    "\n",
    "with open('flat_triggers.pickle', 'rb') as f:\n",
    "    my_triggers = pickle.load(f)\n",
    "   \n",
    "count = sum(1 for entry in my_triggers if start_time <= entry[0][0] <= end_time)\n",
    "\n",
    "print(f\"Number of triggers between {start_time} and {end_time}: {count}\")\n",
    "starts = [entry[0][0] for entry in my_triggers if start_time <= entry[0][0] <= end_time]\n",
    "peaks = [entry[2] for entry in my_triggers if start_time <= entry[0][0] <= end_time and entry[2] == 1]\n",
    "print(len(peaks))\n",
    "print(peaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591c0d2-f97c-42fb-b72f-6086645bde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/lydiagk/obs_tremor/tremor_detection/results\")\n",
    "\n",
    "# define time window\n",
    "start_time_v = UTCDateTime(\"2017-09-01T00:00:00.000\")\n",
    "end_time_v = UTCDateTime(\"2017-09-08T00:00:00.000\")\n",
    "\n",
    "with open('HYSB1_HHN_3-10Hz_classifications.pickle', 'rb') as f:\n",
    "    my_triggers_v = pickle.load(f)\n",
    "\n",
    "\n",
    "count_v = sum(1 for entry in my_triggers_v if start_time_v <= entry[0][0] <= end_time_v)\n",
    "\n",
    "print(f\"Number of triggers between {start_time_v} and {end_time_v}: {count_v}\")\n",
    "starts_v = [entry[0][0] for entry in my_triggers_v if start_time_v <= entry[0][0] <= end_time_v]\n",
    "peaks_v = [entry[2] for entry in my_triggers_v if start_time <= entry[0][0] <= end_time and entry[2] == 1]\n",
    "\n",
    "print(len(peaks_v))\n",
    "print(peaks_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e56af-cb64-45f0-b983-2cbdfa37c1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb08bc4-a809-45df-8e58-a208a3a7562c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo 3.10 (SHARED)",
   "language": "python",
   "name": "seismo-py310-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
